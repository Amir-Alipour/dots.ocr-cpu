{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyP4grMs49ThQbJf5snN4z0e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Clone the repository\n","!git clone https://github.com/rednote-hilab/dots.ocr.git\n","\n","%cd dots.ocr"],"metadata":{"id":"ojyr2tTI6fQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Delete flash-attn from requirements\n","!sed -i '/flash-attn/d' requirements.txt"],"metadata":{"id":"q4c2uuhoJmyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Install dependencies\n","!pip install torch torchvision torchaudio\n","!pip install -e ."],"metadata":{"id":"EzuPx4Kv7kol"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Download dots.ocr model\n","!python3 tools/download_model.py"],"metadata":{"id":"dNkhSzx36jH3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from transformers import AutoModelForCausalLM, AutoProcessor\n","from qwen_vl_utils import process_vision_info\n","\n","# --- Load the model with CPU-specific configurations ---\n","model_path = \"./weights/DotsOCR\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    attn_implementation=\"sdpa\",  # Change 1: Use \"sdpa\" instead of \"flash_attention_2\"\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"cpu\",            # Change 2: Explicitly set the device to \"cpu\"\n","    trust_remote_code=True\n",")\n","processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)\n","\n","# --- Prepare your image and prompt ---\n","image_path = \"/content/example.jpg\" # Or any other image path\n","prompt = \"\"\"Extract texts from image\"\"\" # Your prompt here\n","\n","messages = [\n","    {\n","        \"role\": \"user\",\n","        \"content\": [\n","            { \"type\": \"image\", \"image\": image_path },\n","            {\"type\": \"text\", \"text\": prompt}\n","        ]\n","    }\n","]\n","\n","# --- Standard processing steps ---\n","text = processor.apply_chat_template(\n","    messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","image_inputs, video_inputs = process_vision_info(messages)\n","inputs = processor(\n","    text=[text],\n","    images=image_inputs,\n","    videos=video_inputs,\n","    padding=True,\n","    return_tensors=\"pt\",\n",")\n","\n","# --- Inference ---\n","generated_ids = model.generate(**inputs, max_new_tokens=128)\n","generated_ids_trimmed = [\n","    out_ids[len(in_ids):]\n","    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n","]\n","output_text = processor.batch_decode(\n","    generated_ids_trimmed,\n","    skip_special_tokens=True,\n","    clean_up_tokenization_spaces=False\n",")\n","print(output_text)"],"metadata":{"id":"wSXprSXv6uz6"},"execution_count":null,"outputs":[]}]}